{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "085450cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helpMe\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "device = helpMe.get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb276",
   "metadata": {},
   "source": [
    "## Configrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec87bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"UNet_8\"\n",
    "image_size = 8\n",
    "batch_size = 32\n",
    "# z_dim = 128\n",
    "# DATA_DIR = './imageNet_lp/torch_image_folder/mnt/volume_sfo3_01/imagenet-lt/ImageDataset/train'\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "channels =1\n",
    "epochs = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "762823f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[64, 128]):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.bottleneck = nn.Conv2d(features[-1], features[-1]*2, kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsampling part\n",
    "        for feature in features:\n",
    "            self.downs.append(self.conv_block(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Upsampling part\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(self.conv_block(feature*2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip_connection.shape[2:])\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea7a3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, features=[64, 128]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        in_features = features[0]\n",
    "        for feature in features[1:]:\n",
    "            self.conv_layers.append(self._block(in_features, feature, stride=2))\n",
    "            in_features = feature\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_features, 1, kernel_size=2, stride=1, padding=0)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.LeakyReLU(0.2, inplace=True)(self.conv1(x))\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            # print(x.shape)\n",
    "            \n",
    "        return torch.sigmoid(self.final_conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6543d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "G,D = UNetGenerator(), Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55d46390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in G: 1273153 D: 133057\n"
     ]
    }
   ],
   "source": [
    "print('Number of params in G: {} D: {}'.format(\n",
    "*[sum([p.data.nelement() for p in net.parameters()]) for net in [G,D]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c47a5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = Datasets.MNIST(root='./Datasxts/MNIST/', train=True, download=True,transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6b8df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def to_gaus(imgs):\n",
    "    smoothed_imgs = []\n",
    "    higher_freq = []\n",
    "\n",
    "    for img_tensor in imgs:\n",
    "\n",
    "        img = T.ToPILImage()(img_tensor)\n",
    "         \n",
    "        S_img = img.filter(ImageFilter.GaussianBlur(radius=1))  # Adjust the radius as needed\n",
    "              \n",
    "        # H_img = T.ToPILImage()(H_img)\n",
    "        S_img = T.ToTensor()(S_img)\n",
    "        H_img = img_tensor - S_img\n",
    "        higher_freq.append(H_img)\n",
    "        smoothed_imgs.append(S_img)\n",
    "\n",
    "    smoothed_imgs = torch.stack(smoothed_imgs)\n",
    "    higher_freq = torch.stack(higher_freq)\n",
    "    torchvision.utils.save_image(smoothed_imgs.detach(), f\"smoooooo_8.png\", normalize=True,nrow=8)\n",
    "    return smoothed_imgs,higher_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e71f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_generated_images(genH_realH, recon, epoch,i, path, device):\n",
    "    os.makedirs(f\"{path}Generated\", exist_ok=True)\n",
    "    torchvision.utils.save_image(genH_realH.detach(), f\"{path}Generated/{epoch}_{i}_generated_images_epoch.png\", normalize=True,nrow=8)\n",
    "    torchvision.utils.save_image(recon.detach(), f\"{path}Generated/{epoch}_{i}_generated_recon_epoch.png\", normalize=True,nrow=8)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5adcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, 32, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aba6675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/110]: 100%|██████████| 1875/1875 [00:58<00:00, 32.24it/s, D_loss=0.583, G_loss=7.934] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/110] Loss D: 0.5633, loss G: 11.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/110]: 100%|██████████| 1875/1875 [00:56<00:00, 32.97it/s, D_loss=0.664, G_loss=5.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/110] Loss D: 0.6322, loss G: 6.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/110]: 100%|██████████| 1875/1875 [00:55<00:00, 33.56it/s, D_loss=0.757, G_loss=4.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/110] Loss D: 0.6558, loss G: 5.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/110]: 100%|██████████| 1875/1875 [00:57<00:00, 32.49it/s, D_loss=0.587, G_loss=3.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/110] Loss D: 0.6623, loss G: 4.3553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/110]: 100%|██████████| 1875/1875 [00:56<00:00, 33.09it/s, D_loss=0.707, G_loss=4.170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/110] Loss D: 0.6681, loss G: 3.9533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/110]: 100%|██████████| 1875/1875 [00:56<00:00, 33.15it/s, D_loss=0.599, G_loss=4.150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/110] Loss D: 0.6699, loss G: 3.6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/110]:  97%|█████████▋| 1815/1875 [00:53<00:01, 33.64it/s, D_loss=0.724, G_loss=3.214]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 109\u001b[0m\n\u001b[0;32m    105\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    107\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 47\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, dataloader, num_epochs, batch_size, checkpoint_dir)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train Discriminator\u001b[39;00m\n\u001b[0;32m     46\u001b[0m opt_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 47\u001b[0m output_real \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_high_freqs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m loss_disc_real \u001b[38;5;241m=\u001b[39m criterion(output_real, torch\u001b[38;5;241m.\u001b[39mones_like(output_real))\n\u001b[0;32m     49\u001b[0m generated_high_freqs \u001b[38;5;241m=\u001b[39m generator(smoothed_images)\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 27\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AMP Scalers\n",
    "scaler_G = GradScaler()\n",
    "scaler_D = GradScaler()\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, batch_size, checkpoint_dir=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_disc = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    criterion = nn.BCELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    " \n",
    "    \n",
    "    start_epoch = 1\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "            opt_disc.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "            scaler_G.load_state_dict(checkpoint['scaler_G'])\n",
    "            scaler_D.load_state_dict(checkpoint['scaler_D'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        total_d_loss = 0.0\n",
    "        total_g_loss = 0.0\n",
    "        \n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader)) as t:\n",
    "            for i, (images, _) in t:\n",
    "                smoothed_images, real_high_freqs = to_gaus(imgs=images)\n",
    "                smoothed_images = smoothed_images.to(device)\n",
    "                real_high_freqs = real_high_freqs.to(device)\n",
    "\n",
    "                # Train Discriminator\n",
    "                opt_disc.zero_grad()\n",
    "                output_real = discriminator(real_high_freqs).view(-1)\n",
    "                loss_disc_real = criterion(output_real, torch.ones_like(output_real))\n",
    "                generated_high_freqs = generator(smoothed_images)\n",
    "                output_fake = discriminator(generated_high_freqs.detach()).view(-1)\n",
    "                loss_disc_fake = criterion(output_fake, torch.zeros_like(output_fake))\n",
    "                loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "                loss_disc.backward()\n",
    "                opt_disc.step()\n",
    "\n",
    "                total_d_loss += loss_disc.item()\n",
    "\n",
    "                # Train Generator\n",
    "                opt_gen.zero_grad()\n",
    "                output_fake = discriminator(generated_high_freqs).view(-1)\n",
    "                loss_gen = criterion(output_fake, torch.ones_like(output_fake))\n",
    "                loss_l1 = l1_loss(generated_high_freqs, real_high_freqs)\n",
    "                loss_generator = loss_gen + 100 * loss_l1\n",
    "                loss_generator.backward()\n",
    "                opt_gen.step()\n",
    "\n",
    "                total_g_loss += loss_generator.item()\n",
    "                \n",
    "                t.set_description(f'Epoch [{epoch}/{num_epochs}]')\n",
    "                t.set_postfix({'D_loss': f'{loss_disc:.3f}',\n",
    "                               'G_loss': f'{loss_generator:.3f}'})\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    recon_imgs = smoothed_images + generated_high_freqs\n",
    "                    save_generated_images(torch.cat([real_high_freqs,generated_high_freqs],dim=0),torch.cat([images.to(device),recon_imgs],dim=0), epoch,i, checkpoint_dir, device)\n",
    "                \n",
    "                del images,smoothed_images, real_high_freqs, generated_high_freqs, output_real, output_fake, loss_disc_real, loss_disc_fake, loss_disc, loss_gen, loss_l1, loss_generator\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        avg_d_loss = total_d_loss / len(dataloader)\n",
    "        avg_g_loss = total_g_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Loss D: {avg_d_loss:.4f}, loss G: {avg_g_loss:.4f}\")\n",
    "\n",
    "        # Save generated images\n",
    "\n",
    "        # Save the model\n",
    "        save_model(generator, discriminator, opt_gen, opt_disc, epoch, checkpoint_dir)\n",
    "        \n",
    "def save_model(generator, discriminator, opt_gen, opt_disc, epoch, checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': opt_gen.state_dict(),\n",
    "        'optimizer_D_state_dict': opt_disc.state_dict(),\n",
    "        'scaler_G': scaler_G.state_dict(),\n",
    "        'scaler_D': scaler_D.state_dict()\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "generator = UNetGenerator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "checkpoint_dir = f\"Models/{model_name}/\"\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, num_epochs =epochs, batch_size=batch_size, checkpoint_dir=checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de475901",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "a,n= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475385c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gaus(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
