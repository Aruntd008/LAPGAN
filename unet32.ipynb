{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085450cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpMe\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "device = helpMe.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb276",
   "metadata": {},
   "source": [
    "## Configrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"UNet\"\n",
    "image_size = 32\n",
    "batch_size = 32\n",
    "# z_dim = 128\n",
    "# DATA_DIR = './imageNet_lp/torch_image_folder/mnt/volume_sfo3_01/imagenet-lt/ImageDataset/train'\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "channels =1\n",
    "epochs = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762823f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, image_size = 32, num_classes=10, features=[64, 128, 256, 512]):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, image_size * image_size)  # Assuming square images\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.bottleneck = nn.Conv2d(features[-1], features[-1] * 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.downs.append(self.conv_block(in_channels+1, features[0]))\n",
    "        in_channels = features[0]\n",
    "        # Downsampling part\n",
    "        for feature in features[1:]:\n",
    "            self.downs.append(self.conv_block(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Upsampling part\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(self.conv_block(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Embedding labels\n",
    "        label_embeddings = self.label_embedding(labels).view(labels.size(0), self.in_channels, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, label_embeddings], dim=1)\n",
    "        \n",
    "        # x = nn.LeakyReLU(0.2, inplace=True)(self.conv1(x))\n",
    "\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = nn.MaxPool2d(kernel_size=2, stride=2)(  x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip_connection.shape[2:])\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b897064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "12.1\n",
      "8801\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, image_size=32, num_classes=10, features=[64, 128, 256, 512]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, image_size * image_size)  # Assuming square images\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        in_features = features[0]\n",
    "        for feature in features[1:]:\n",
    "            self.conv_layers.append(self._block(in_features, feature, stride=2))\n",
    "            in_features = feature\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_features, 1, kernel_size=2, stride=1, padding=0)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # Embedding labels\n",
    "        # print('labels size', labels.shape)\n",
    "        label_embeddings = self.label_embedding(labels).view(labels.size(0), self.in_channels, x.size(2), x.size(3))\n",
    "        print(\"embedding shape\", label_embeddings.shape)\n",
    "        x = torch.cat([x, label_embeddings], dim=1)\n",
    "        print(\"x sahpe\", x.shape)\n",
    "        x = nn.LeakyReLU(0.2, inplace=True)(self.conv1(x))\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return torch.sigmoid(self.final_conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6543d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "G,D = UNetGenerator(), Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d46390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of params in G: {} D: {}'.format(\n",
    "*[sum([p.data.nelement() for p in net.parameters()]) for net in [G,D]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize(32),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = Datasets.MNIST(root='./Datasxts/MNIST/', train=True, download=True,transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def to_gaus(imgs):\n",
    "    smoothed_imgs = []\n",
    "    higher_freq = []\n",
    "\n",
    "    for img_tensor in imgs:\n",
    "\n",
    "        img = T.ToPILImage()(img_tensor)\n",
    "         \n",
    "        S_img = img.filter(ImageFilter.GaussianBlur(radius=4))  # Adjust the radius as needed\n",
    "              \n",
    "        # H_img = T.ToPILImage()(H_img)\n",
    "        S_img = T.ToTensor()(S_img)\n",
    "        H_img = img_tensor - S_img\n",
    "        higher_freq.append(H_img)\n",
    "        smoothed_imgs.append(S_img)\n",
    "\n",
    "    smoothed_imgs = torch.stack(smoothed_imgs)\n",
    "    higher_freq = torch.stack(higher_freq)\n",
    "    torchvision.utils.save_image(smoothed_imgs.detach(), f\"smoooooo.png\", normalize=True,nrow=8)\n",
    "    return smoothed_imgs,higher_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_generated_images(genH_realH, recon, epoch,i, path):\n",
    "    os.makedirs(f\"{path}Generated\", exist_ok=True)\n",
    "    torchvision.utils.save_image(genH_realH.detach(), f\"{path}Generated/{epoch}_{i}_generated_images_epoch.png\", normalize=True,nrow=8)\n",
    "    torchvision.utils.save_image(recon.detach(), f\"{path}Generated/{epoch}_{i}_generated_recon_epoch.png\", normalize=True,nrow=8)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMP Scalers\n",
    "scaler_G = GradScaler()\n",
    "scaler_D = GradScaler()\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, batch_size, checkpoint_dir=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_disc = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    criterion = nn.BCELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    "    start_epoch = 1\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "            opt_disc.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "            scaler_G.load_state_dict(checkpoint['scaler_G'])\n",
    "            scaler_D.load_state_dict(checkpoint['scaler_D'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        total_d_loss = 0.0\n",
    "        total_g_loss = 0.0\n",
    "        \n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader)) as t:\n",
    "            for i, (images, labels) in t:\n",
    "                smoothed_images, real_high_freqs = to_gaus(imgs=images)\n",
    "                smoothed_images = smoothed_images.to(device)\n",
    "                real_high_freqs = real_high_freqs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Train Discriminator\n",
    "                opt_disc.zero_grad()\n",
    "                output_real = discriminator(real_high_freqs, labels).view(-1)\n",
    "                loss_disc_real = criterion(output_real, torch.ones_like(output_real))\n",
    "                generated_high_freqs = generator(smoothed_images, labels)\n",
    "                output_fake = discriminator(generated_high_freqs.detach(), labels).view(-1)\n",
    "                loss_disc_fake = criterion(output_fake, torch.zeros_like(output_fake))\n",
    "                loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "                loss_disc.backward()\n",
    "                opt_disc.step()\n",
    "\n",
    "                total_d_loss += loss_disc.item()\n",
    "\n",
    "                # Train Generator\n",
    "                opt_gen.zero_grad()\n",
    "                output_fake = discriminator(generated_high_freqs, labels).view(-1)\n",
    "                loss_gen = criterion(output_fake, torch.ones_like(output_fake))\n",
    "                loss_l1 = l1_loss(generated_high_freqs, real_high_freqs)\n",
    "                loss_generator = loss_gen + 100 * loss_l1\n",
    "                loss_generator.backward()\n",
    "                opt_gen.step()\n",
    "\n",
    "                total_g_loss += loss_generator.item()\n",
    "                \n",
    "                t.set_description(f'Epoch [{epoch}/{num_epochs}]')\n",
    "                t.set_postfix({'D_loss': f'{loss_disc:.3f}', 'G_loss': f'{loss_generator:.3f}'})\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    recon_imgs = smoothed_images + generated_high_freqs\n",
    "                    save_generated_images(torch.cat([real_high_freqs, generated_high_freqs], dim=0), torch.cat([images.to(device), recon_imgs], dim=0), epoch, i, checkpoint_dir)\n",
    "                \n",
    "                del images, smoothed_images, real_high_freqs, generated_high_freqs, output_real, output_fake, loss_disc_real, loss_disc_fake, loss_disc, loss_gen, loss_l1, loss_generator\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            \n",
    "        avg_d_loss = total_d_loss / len(dataloader)\n",
    "        avg_g_loss = total_g_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Loss D: {avg_d_loss:.4f}, loss G: {avg_g_loss:.4f}\")\n",
    "\n",
    "        # Save the model\n",
    "        save_model(generator, discriminator, opt_gen, opt_disc, epoch, checkpoint_dir)\n",
    "\n",
    "        \n",
    "def save_model(generator, discriminator, opt_gen, opt_disc, epoch, checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': opt_gen.state_dict(),\n",
    "        'optimizer_D_state_dict': opt_disc.state_dict(),\n",
    "        'scaler_G': scaler_G.state_dict(),\n",
    "        'scaler_D': scaler_D.state_dict()\n",
    "    }, checkpoint_path)\n",
    "\n",
    "dataloader = DataLoader(dataset, 32, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "generator = UNetGenerator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "model_name = \"unet\"\n",
    "checkpoint_dir = f\"Models/{model_name}_w/\"\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, num_epochs =epochs, batch_size=32, checkpoint_dir=checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de475901",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "a,n= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475385c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gaus(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e641c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
