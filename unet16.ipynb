{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085450cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helpMe\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "device = helpMe.get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb276",
   "metadata": {},
   "source": [
    "## Configrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec87bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"UNet_16\"\n",
    "image_size = 16\n",
    "batch_size = 32\n",
    "# z_dim = 128\n",
    "# DATA_DIR = './imageNet_lp/torch_image_folder/mnt/volume_sfo3_01/imagenet-lt/ImageDataset/train'\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "channels =1\n",
    "epochs = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762823f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256]):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.bottleneck = nn.Conv2d(features[-1], features[-1]*2, kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsampling part\n",
    "        for feature in features:\n",
    "            self.downs.append(self.conv_block(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Upsampling part\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(self.conv_block(feature*2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip_connection.shape[2:])\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7a3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, features=[64, 128, 256]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        in_features = features[0]\n",
    "        for feature in features[1:]:\n",
    "            self.conv_layers.append(self._block(in_features, feature, stride=2))\n",
    "            in_features = feature\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_features, 1, kernel_size=2, stride=1, padding=0)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.LeakyReLU(0.2, inplace=True)(self.conv1(x))\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            # print(x.shape)\n",
    "            \n",
    "        return torch.sigmoid(self.final_conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6543d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "G,D = UNetGenerator(), Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d46390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in G: 5339969 D: 658625\n"
     ]
    }
   ],
   "source": [
    "print('Number of params in G: {} D: {}'.format(\n",
    "*[sum([p.data.nelement() for p in net.parameters()]) for net in [G,D]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c47a5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = Datasets.MNIST(root='./Datasxts/MNIST/', train=True, download=True,transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b8df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def to_gaus(imgs):\n",
    "    smoothed_imgs = []\n",
    "    higher_freq = []\n",
    "\n",
    "    for img_tensor in imgs:\n",
    "\n",
    "        img = T.ToPILImage()(img_tensor)\n",
    "         \n",
    "        S_img = img.filter(ImageFilter.GaussianBlur(radius=4))  # Adjust the radius as needed\n",
    "              \n",
    "        # H_img = T.ToPILImage()(H_img)\n",
    "        S_img = T.ToTensor()(S_img)\n",
    "        H_img = img_tensor - S_img\n",
    "        higher_freq.append(H_img)\n",
    "        smoothed_imgs.append(S_img)\n",
    "\n",
    "    smoothed_imgs = torch.stack(smoothed_imgs)\n",
    "    higher_freq = torch.stack(higher_freq)\n",
    "    torchvision.utils.save_image(smoothed_imgs.detach(), f\"smoooooo_16.png\", normalize=True,nrow=8)\n",
    "    return smoothed_imgs,higher_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e71f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_generated_images(genH_realH, recon, epoch,i, path, device):\n",
    "    os.makedirs(f\"{path}Generated\", exist_ok=True)\n",
    "    torchvision.utils.save_image(genH_realH.detach(), f\"{path}Generated/{epoch}_{i}_generated_images_epoch.png\", normalize=True,nrow=8)\n",
    "    torchvision.utils.save_image(recon.detach(), f\"{path}Generated/{epoch}_{i}_generated_recon_epoch.png\", normalize=True,nrow=8)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba6675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/110]: 100%|██████████| 1875/1875 [01:02<00:00, 29.85it/s, D_loss=0.473, G_loss=10.611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/110] Loss D: 0.5262, loss G: 14.6226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/110]:   0%|          | 2/1875 [00:03<51:13,  1.64s/it, D_loss=0.614, G_loss=12.612]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 109\u001b[0m\n\u001b[0;32m    105\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    107\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 67\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, dataloader, num_epochs, batch_size, checkpoint_dir)\u001b[0m\n\u001b[0;32m     64\u001b[0m loss_generator\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     65\u001b[0m opt_gen\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 67\u001b[0m total_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m t\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_disc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     71\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_generator\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AMP Scalers\n",
    "scaler_G = GradScaler()\n",
    "scaler_D = GradScaler()\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, batch_size, checkpoint_dir=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_disc = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    criterion = nn.BCELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    " \n",
    "    \n",
    "    start_epoch = 1\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "            opt_disc.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "            scaler_G.load_state_dict(checkpoint['scaler_G'])\n",
    "            scaler_D.load_state_dict(checkpoint['scaler_D'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        total_d_loss = 0.0\n",
    "        total_g_loss = 0.0\n",
    "        \n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader)) as t:\n",
    "            for i, (images, _) in t:\n",
    "                smoothed_images, real_high_freqs = to_gaus(imgs=images)\n",
    "                smoothed_images = smoothed_images.to(device)\n",
    "                real_high_freqs = real_high_freqs.to(device)\n",
    "\n",
    "                # Train Discriminator\n",
    "                opt_disc.zero_grad()\n",
    "                output_real = discriminator(real_high_freqs).view(-1)\n",
    "                loss_disc_real = criterion(output_real, torch.ones_like(output_real))\n",
    "                generated_high_freqs = generator(smoothed_images)\n",
    "                output_fake = discriminator(generated_high_freqs.detach()).view(-1)\n",
    "                loss_disc_fake = criterion(output_fake, torch.zeros_like(output_fake))\n",
    "                loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "                loss_disc.backward()\n",
    "                opt_disc.step()\n",
    "\n",
    "                total_d_loss += loss_disc.item()\n",
    "\n",
    "                # Train Generator\n",
    "                opt_gen.zero_grad()\n",
    "                output_fake = discriminator(generated_high_freqs).view(-1)\n",
    "                loss_gen = criterion(output_fake, torch.ones_like(output_fake))\n",
    "                loss_l1 = l1_loss(generated_high_freqs, real_high_freqs)\n",
    "                loss_generator = loss_gen + 100 * loss_l1\n",
    "                loss_generator.backward()\n",
    "                opt_gen.step()\n",
    "\n",
    "                total_g_loss += loss_generator.item()\n",
    "                \n",
    "                t.set_description(f'Epoch [{epoch}/{num_epochs}]')\n",
    "                t.set_postfix({'D_loss': f'{loss_disc:.3f}',\n",
    "                               'G_loss': f'{loss_generator:.3f}'})\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    recon_imgs = smoothed_images + generated_high_freqs\n",
    "                    save_generated_images(torch.cat([real_high_freqs,generated_high_freqs],dim=0),torch.cat([images.to(device),recon_imgs],dim=0), epoch,i, checkpoint_dir, device)\n",
    "                \n",
    "                del images,smoothed_images, real_high_freqs, generated_high_freqs, output_real, output_fake, loss_disc_real, loss_disc_fake, loss_disc, loss_gen, loss_l1, loss_generator\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        avg_d_loss = total_d_loss / len(dataloader)\n",
    "        avg_g_loss = total_g_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Loss D: {avg_d_loss:.4f}, loss G: {avg_g_loss:.4f}\")\n",
    "\n",
    "        # Save generated images\n",
    "\n",
    "        # Save the model\n",
    "        save_model(generator, discriminator, opt_gen, opt_disc, epoch, checkpoint_dir)\n",
    "        \n",
    "def save_model(generator, discriminator, opt_gen, opt_disc, epoch, checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': opt_gen.state_dict(),\n",
    "        'optimizer_D_state_dict': opt_disc.state_dict(),\n",
    "        'scaler_G': scaler_G.state_dict(),\n",
    "        'scaler_D': scaler_D.state_dict()\n",
    "    }, checkpoint_path)\n",
    "\n",
    "dataloader = DataLoader(dataset, 32, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "generator = UNetGenerator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "checkpoint_dir = f\"Models/{model_name}/\"\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, num_epochs =epochs, batch_size=batch_size, checkpoint_dir=checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de475901",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "a,n= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475385c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0235, 0.0353, 0.0431,  ..., 0.0471, 0.0392, 0.0314],\n",
       "           [0.0275, 0.0392, 0.0471,  ..., 0.0510, 0.0431, 0.0353],\n",
       "           [0.0314, 0.0431, 0.0549,  ..., 0.0588, 0.0471, 0.0392],\n",
       "           ...,\n",
       "           [0.0392, 0.0510, 0.0667,  ..., 0.0627, 0.0510, 0.0392],\n",
       "           [0.0353, 0.0471, 0.0627,  ..., 0.0588, 0.0471, 0.0353],\n",
       "           [0.0353, 0.0471, 0.0588,  ..., 0.0549, 0.0471, 0.0353]]],\n",
       " \n",
       " \n",
       "         [[[0.0353, 0.0431, 0.0471,  ..., 0.0431, 0.0353, 0.0314],\n",
       "           [0.0431, 0.0471, 0.0549,  ..., 0.0471, 0.0431, 0.0353],\n",
       "           [0.0471, 0.0549, 0.0627,  ..., 0.0549, 0.0471, 0.0392],\n",
       "           ...,\n",
       "           [0.0353, 0.0431, 0.0549,  ..., 0.0863, 0.0745, 0.0667],\n",
       "           [0.0275, 0.0353, 0.0471,  ..., 0.0784, 0.0706, 0.0627],\n",
       "           [0.0235, 0.0314, 0.0392,  ..., 0.0745, 0.0667, 0.0588]]],\n",
       " \n",
       " \n",
       "         [[[0.0235, 0.0314, 0.0392,  ..., 0.0980, 0.0902, 0.0824],\n",
       "           [0.0275, 0.0353, 0.0471,  ..., 0.1059, 0.0980, 0.0902],\n",
       "           [0.0353, 0.0431, 0.0549,  ..., 0.1137, 0.1020, 0.0941],\n",
       "           ...,\n",
       "           [0.0784, 0.0941, 0.1059,  ..., 0.0824, 0.0706, 0.0549],\n",
       "           [0.0745, 0.0902, 0.1020,  ..., 0.0706, 0.0588, 0.0471],\n",
       "           [0.0706, 0.0824, 0.0941,  ..., 0.0627, 0.0549, 0.0431]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.0588, 0.0667, 0.0745,  ..., 0.0510, 0.0471, 0.0353],\n",
       "           [0.0627, 0.0706, 0.0784,  ..., 0.0627, 0.0510, 0.0431],\n",
       "           [0.0667, 0.0745, 0.0863,  ..., 0.0706, 0.0627, 0.0510],\n",
       "           ...,\n",
       "           [0.0667, 0.0745, 0.0863,  ..., 0.0863, 0.0745, 0.0667],\n",
       "           [0.0627, 0.0706, 0.0824,  ..., 0.0745, 0.0667, 0.0588],\n",
       "           [0.0627, 0.0706, 0.0784,  ..., 0.0667, 0.0588, 0.0510]]],\n",
       " \n",
       " \n",
       "         [[[0.0235, 0.0314, 0.0392,  ..., 0.0471, 0.0392, 0.0314],\n",
       "           [0.0275, 0.0353, 0.0431,  ..., 0.0510, 0.0392, 0.0353],\n",
       "           [0.0275, 0.0392, 0.0471,  ..., 0.0549, 0.0471, 0.0392],\n",
       "           ...,\n",
       "           [0.0431, 0.0510, 0.0667,  ..., 0.0588, 0.0471, 0.0353],\n",
       "           [0.0392, 0.0510, 0.0627,  ..., 0.0510, 0.0431, 0.0314],\n",
       "           [0.0392, 0.0471, 0.0588,  ..., 0.0471, 0.0392, 0.0275]]],\n",
       " \n",
       " \n",
       "         [[[0.0314, 0.0392, 0.0510,  ..., 0.1176, 0.1059, 0.0941],\n",
       "           [0.0353, 0.0431, 0.0588,  ..., 0.1255, 0.1098, 0.1020],\n",
       "           [0.0431, 0.0549, 0.0706,  ..., 0.1333, 0.1176, 0.1059],\n",
       "           ...,\n",
       "           [0.1059, 0.1176, 0.1294,  ..., 0.0941, 0.0784, 0.0667],\n",
       "           [0.0980, 0.1137, 0.1216,  ..., 0.0824, 0.0706, 0.0588],\n",
       "           [0.0941, 0.1059, 0.1176,  ..., 0.0745, 0.0627, 0.0549]]]]),\n",
       " tensor([[[[-1.0235, -1.0353, -1.0431,  ..., -1.0471, -1.0392, -1.0314],\n",
       "           [-1.0275, -1.0392, -1.0471,  ..., -1.0510, -1.0431, -1.0353],\n",
       "           [-1.0314, -1.0431, -1.0549,  ..., -1.0588, -1.0471, -1.0392],\n",
       "           ...,\n",
       "           [-1.0392, -1.0510, -1.0667,  ..., -1.0627, -1.0510, -1.0392],\n",
       "           [-1.0353, -1.0471, -1.0627,  ..., -1.0588, -1.0471, -1.0353],\n",
       "           [-1.0353, -1.0471, -1.0588,  ..., -1.0549, -1.0471, -1.0353]]],\n",
       " \n",
       " \n",
       "         [[[-1.0353, -1.0431, -1.0471,  ..., -1.0431, -1.0353, -1.0314],\n",
       "           [-1.0431, -1.0471, -1.0549,  ..., -1.0471, -1.0431, -1.0353],\n",
       "           [-1.0471, -1.0549, -1.0627,  ..., -1.0549, -1.0471, -1.0392],\n",
       "           ...,\n",
       "           [-1.0353, -1.0431, -1.0549,  ..., -1.0863, -1.0745, -1.0667],\n",
       "           [-1.0275, -1.0353, -1.0471,  ..., -1.0784, -1.0706, -1.0627],\n",
       "           [-1.0235, -1.0314, -1.0392,  ..., -1.0745, -1.0667, -1.0588]]],\n",
       " \n",
       " \n",
       "         [[[-1.0235, -1.0314, -1.0392,  ..., -1.0980, -1.0902, -1.0824],\n",
       "           [-1.0275, -1.0353, -1.0471,  ..., -1.1059, -1.0980, -1.0902],\n",
       "           [-1.0353, -1.0431, -1.0549,  ..., -1.1137, -1.1020, -1.0941],\n",
       "           ...,\n",
       "           [-1.0784, -1.0941, -1.1059,  ..., -1.0824, -1.0706, -1.0549],\n",
       "           [-1.0745, -1.0902, -1.1020,  ..., -1.0706, -1.0588, -1.0471],\n",
       "           [-1.0706, -1.0824, -1.0941,  ..., -1.0627, -1.0549, -1.0431]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.0588, -1.0667, -1.0745,  ..., -1.0510, -1.0471, -1.0353],\n",
       "           [-1.0627, -1.0706, -1.0784,  ..., -1.0627, -1.0510, -1.0431],\n",
       "           [-1.0667, -1.0745, -1.0863,  ..., -1.0706, -1.0627, -1.0510],\n",
       "           ...,\n",
       "           [-1.0667, -1.0745, -1.0863,  ..., -1.0863, -1.0745, -1.0667],\n",
       "           [-1.0627, -1.0706, -1.0824,  ..., -1.0745, -1.0667, -1.0588],\n",
       "           [-1.0627, -1.0706, -1.0784,  ..., -1.0667, -1.0588, -1.0510]]],\n",
       " \n",
       " \n",
       "         [[[-1.0235, -1.0314, -1.0392,  ..., -1.0471, -1.0392, -1.0314],\n",
       "           [-1.0275, -1.0353, -1.0431,  ..., -1.0510, -1.0392, -1.0353],\n",
       "           [-1.0275, -1.0392, -1.0471,  ..., -1.0549, -1.0471, -1.0392],\n",
       "           ...,\n",
       "           [-1.0431, -1.0510, -1.0667,  ..., -1.0588, -1.0471, -1.0353],\n",
       "           [-1.0392, -1.0510, -1.0627,  ..., -1.0510, -1.0431, -1.0314],\n",
       "           [-1.0392, -1.0471, -1.0588,  ..., -1.0471, -1.0392, -1.0275]]],\n",
       " \n",
       " \n",
       "         [[[-1.0314, -1.0392, -1.0510,  ..., -1.1176, -1.1059, -1.0941],\n",
       "           [-1.0353, -1.0431, -1.0588,  ..., -1.1255, -1.1098, -1.1020],\n",
       "           [-1.0431, -1.0549, -1.0706,  ..., -1.1333, -1.1176, -1.1059],\n",
       "           ...,\n",
       "           [-1.1059, -1.1176, -1.1294,  ..., -1.0941, -1.0784, -1.0667],\n",
       "           [-1.0980, -1.1137, -1.1216,  ..., -1.0824, -1.0706, -1.0588],\n",
       "           [-1.0941, -1.1059, -1.1176,  ..., -1.0745, -1.0627, -1.0549]]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_gaus(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e641c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
