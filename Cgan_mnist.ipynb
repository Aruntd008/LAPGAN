{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:08:36.270407Z",
     "start_time": "2024-05-21T14:08:36.257078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import functools\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torchvision.utils as vu\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helpMe\n",
    "import layers\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] =1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0650fd",
   "metadata": {},
   "source": [
    "## Configrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1b0ac896a30c2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:08.985090Z",
     "start_time": "2024-05-21T14:09:08.981890Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Cgan_mnist\"\n",
    "image_size = 32\n",
    "batch_size = 100\n",
    "z_dim = 128\n",
    "# DATA_DIR = './imageNet_lp/torch_image_folder/mnt/volume_sfo3_01/imagenet-lt/ImageDataset/train'\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "channels =1\n",
    "epochs = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3797c13097093dfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:08.981890Z",
     "start_time": "2024-05-21T14:08:36.333364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maruntd008\u001b[0m (\u001b[33maruntd08\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13a23c1fc8f406d990e996d7390e487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Arun\\pytorch\\expriements\\LAPGAN\\wandb\\run-20240527_152652-8k75d6ds</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruntd08/Cgan_mnist/runs/8k75d6ds' target=\"_blank\">zesty-energy-6</a></strong> to <a href='https://wandb.ai/aruntd08/Cgan_mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruntd08/Cgan_mnist' target=\"_blank\">https://wandb.ai/aruntd08/Cgan_mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruntd08/Cgan_mnist/runs/8k75d6ds' target=\"_blank\">https://wandb.ai/aruntd08/Cgan_mnist/runs/8k75d6ds</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aruntd08/Cgan_mnist/runs/8k75d6ds?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a90c9a38d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Cgan_mnist\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"architecture\": \"Gan\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f87cf3",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f02b5176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.659337Z",
     "start_time": "2024-05-21T14:09:08.989201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./Datasxts/MNIST\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforms = T.Compose([T.Resize(image_size), T.CenterCrop(image_size), T.ToTensor(), T.Normalize(*stats)])\n",
    "transforms = T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.RandomRotation(10),  # Random rotation within 10 degrees\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = Datasets.MNIST(root='./Datasxts/MNIST', train=True, download=True,transform=transforms) # cifar-10\n",
    "# dataset = D.ImageFolder(DATA_DIR, transform=transforms) # imageNet_lp\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c89a27223b2800b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.668145Z",
     "start_time": "2024-05-21T14:09:09.663589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Classes: 10\n",
      "Class Names:  ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "class_list=dataset.classes\n",
    "print('No of Classes:',len(class_list))\n",
    "print('Class Names: ',class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8cb87e25802f9f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.741214Z",
     "start_time": "2024-05-21T14:09:09.674364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = helpMe.get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f9e73",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b335024accbfe99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.762896Z",
     "start_time": "2024-05-21T14:09:09.744194Z"
    }
   },
   "outputs": [],
   "source": [
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 which_conv=nn.Conv2d, which_bn=layers.bn, activation=None,\n",
    "                 upsample=None, channel_ratio=4):\n",
    "        super(GBlock, self).__init__()\n",
    "\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.hidden_channels = self.in_channels // channel_ratio\n",
    "        self.which_conv, self.which_bn = which_conv, which_bn\n",
    "        self.activation = activation\n",
    "        # Conv layers\n",
    "        self.conv1 = self.which_conv(self.in_channels, self.hidden_channels,\n",
    "                                     kernel_size=1, padding=0)\n",
    "        self.conv2 = self.which_conv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv3 = self.which_conv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv4 = self.which_conv(self.hidden_channels, self.out_channels,\n",
    "                                     kernel_size=1, padding=0)\n",
    "        # Batchnorm layers\n",
    "        self.bn1 = self.which_bn(self.in_channels)\n",
    "        self.bn2 = self.which_bn(self.hidden_channels)\n",
    "        self.bn3 = self.which_bn(self.hidden_channels)\n",
    "        self.bn4 = self.which_bn(self.hidden_channels)\n",
    "        # upsample layers\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Project down to channel ratio\n",
    "        h = self.conv1(self.activation(self.bn1(x, y)))\n",
    "        # Apply next BN-ReLU\n",
    "        h = self.activation(self.bn2(h, y))\n",
    "        # Drop channels in x if necessary\n",
    "        if self.in_channels != self.out_channels:\n",
    "            x = x[:, :self.out_channels]\n",
    "            # Upsample both h and x at this point\n",
    "        if self.upsample:\n",
    "            h = self.upsample(h)\n",
    "            x = self.upsample(x)\n",
    "        # 3x3 convs\n",
    "        h = self.conv2(h)\n",
    "        h = self.conv3(self.activation(self.bn3(h, y)))\n",
    "        # Final 1x1 conv\n",
    "        h = self.conv4(self.activation(self.bn4(h, y)))\n",
    "        return h + x\n",
    "\n",
    "\n",
    "def G_arch(ch=64, attention='64'):\n",
    "    arch = {}\n",
    "    arch[256] = {'in_channels': [ch * item for item in [16, 16, 8, 8, 4, 2]],\n",
    "                 'out_channels': [ch * item for item in [16, 8, 8, 4, 2, 1]],\n",
    "                 'upsample': [True] * 6,\n",
    "                 'resolution': [8, 16, 32, 64, 128, 256],\n",
    "                 'attention': {2 ** i: (2 ** i in [int(item) for item in attention.split('_')])               #{8: False, 16: False, 32: False, 64: True, 128: False, 256: False}\n",
    "                               for i in range(3, 9)}}                                                         #This dictionary indicates which of the powers of 2 from 2^3 to 2^8 are present in the attention string.\n",
    "    arch[128] = {'in_channels': [ch * item for item in [16, 16, 8, 4, 2]],\n",
    "                 'out_channels': [ch * item for item in [16, 8, 4, 2, 1]],\n",
    "                 'upsample': [True] * 5,\n",
    "                 'resolution': [8, 16, 32, 64, 128],\n",
    "                 'attention': {2 ** i: (2 ** i in [int(item) for item in attention.split('_')])\n",
    "                               for i in range(3, 8)}}\n",
    "    arch[64] = {'in_channels': [ch * item for item in [16, 16, 8, 4]],\n",
    "                'out_channels': [ch * item for item in [16, 8, 4, 2]],\n",
    "                'upsample': [True] * 4,\n",
    "                'resolution': [8, 16, 32, 64],\n",
    "                'attention': {2 ** i: (2 ** i in [int(item) for item in attention.split('_')])\n",
    "                              for i in range(3, 7)}}\n",
    "    arch[32] = {'in_channels': [ch * item for item in [4, 4, 4]],\n",
    "                'out_channels': [ch * item for item in [4, 4, 4]],\n",
    "                'upsample': [True] * 3,\n",
    "                'resolution': [8, 16, 32],\n",
    "                'attention': {2 ** i: (2 ** i in [int(item) for item in attention.split('_')])\n",
    "                              for i in range(3, 6)}}\n",
    "\n",
    "    return arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8f2dd3efb3079e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.776949Z",
     "start_time": "2024-05-21T14:09:09.762896Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, G_ch=64, G_depth=2, dim_z=128, bottom_width=4, resolution=32,\n",
    "                 G_kernel_size=3, G_attn='64', n_classes=1000,\n",
    "                 G_activation=nn.ReLU(inplace=False),\n",
    "                 G_lr=5e-5, G_B1=0.0, G_B2=0.999, adam_eps=1e-8,\n",
    "                 skip_init=False,\n",
    "                 **kwargs):\n",
    "        super(Generator, self).__init__()\n",
    "        # Channel width mulitplier\n",
    "        self.ch = G_ch\n",
    "        # Number of resblocks per stage\n",
    "        self.G_depth = G_depth\n",
    "        # Dimensionality of the latent space\n",
    "        self.dim_z = dim_z\n",
    "        # The initial spatial dimensions\n",
    "        self.bottom_width = bottom_width\n",
    "        # Resolution of the output\n",
    "        self.resolution = resolution\n",
    "        # Kernel size?\n",
    "        self.kernel_size = G_kernel_size\n",
    "        # Attention?\n",
    "        self.attention = G_attn\n",
    "        # number of classes\n",
    "        self.n_classes = n_classes\n",
    "        # Dimensionality of the shared embedding\n",
    "        self.shared_dim = dim_z\n",
    "        # nonlinearity for residual blocks\n",
    "        self.activation = G_activation\n",
    "\n",
    "    \n",
    "        # Architecture dict\n",
    "        self.arch = G_arch(self.ch, self.attention)[resolution]\n",
    "        \n",
    "        # Which convs, batchnorms, and linear layers to use\n",
    "        self.which_conv = functools.partial(layers.SNConv2d,kernel_size=3, padding=1)\n",
    "        self.which_linear = functools.partial(layers.SNLinear)\n",
    "\n",
    "\n",
    "        # We use a non-spectral-normed embedding here regardless;\n",
    "        # For some reason applying SN to G's embedding seems to randomly cripple G\n",
    "        self.which_embedding = nn.Embedding\n",
    "        bn_linear = (functools.partial(self.which_linear, bias=False))\n",
    "        self.which_bn = functools.partial(layers.ccbn,\n",
    "                                          which_linear=bn_linear,\n",
    "                                          input_size=(self.shared_dim + self.dim_z))\n",
    "\n",
    "        # Prepare model\n",
    "        \n",
    "\n",
    "        self.shared = (self.which_embedding(n_classes, self.shared_dim))\n",
    "        # First linear layer\n",
    "        self.linear = self.which_linear(self.dim_z + self.shared_dim,\n",
    "                                        self.arch['in_channels'][0] * (self.bottom_width ** 2))\n",
    "\n",
    "        # self.blocks is a doubly-nested list of modules, the outer loop intended\n",
    "        # to be over blocks at a given resolution (resblocks and/or self-attention)\n",
    "        # while the inner loop is over a given block\n",
    "        \n",
    "        # arch[128] = {'in_channels': [ch * item for item in [16, 16, 8, 4, 2]],\n",
    "        #          'out_channels': [ch * item for item in   [16, 8, 4, 2, 1]],\n",
    "        #          'upsample': [True] * 5,\n",
    "        #          'resolution': [8, 16, 32, 64, 128],\n",
    "        #          'attention': {2 ** i: (2 ** i in [int(item) for item in attention.split('_')])\n",
    "        #                        for i in range(3, 8)}}\n",
    "        self.blocks = []\n",
    "        for index in range(len(self.arch['out_channels'])):\n",
    "            self.blocks += [[GBlock(in_channels=self.arch['in_channels'][index],\n",
    "                                    out_channels=self.arch['in_channels'][index] if g_index == 0 else\n",
    "                                    self.arch['out_channels'][index],\n",
    "                                    which_conv=self.which_conv,\n",
    "                                    which_bn=self.which_bn,\n",
    "                                    activation=self.activation,\n",
    "                                    upsample=(functools.partial(F.interpolate, scale_factor=2)\n",
    "                                              if self.arch['upsample'][index] and g_index == (\n",
    "                                                self.G_depth - 1) else None))]\n",
    "                            for g_index in range(self.G_depth)]\n",
    "\n",
    "            # If attention on this block, attach it to the end\n",
    "            if self.arch['attention'][self.arch['resolution'][index]]:\n",
    "                print('Adding attention layer in G at resolution %d' % self.arch['resolution'][index])\n",
    "                self.blocks[-1] += [layers.Attention(self.arch['out_channels'][index], self.which_conv)]\n",
    "\n",
    "        # Turn self.blocks into a ModuleList so that it's all properly registered.\n",
    "        self.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "        # output layer: batchnorm-relu-conv.\n",
    "        # Consider using a non-spectral conv here\n",
    "        self.output_layer = nn.Sequential(layers.bn(self.arch['out_channels'][-1]),\n",
    "                                          self.activation,\n",
    "                                          self.which_conv(self.arch['out_channels'][-1], channels))\n",
    "\n",
    "        # Initialize weights. Optionally skip init for testing.\n",
    "        if not skip_init:\n",
    "            self.init_weights()\n",
    "\n",
    "        # Set up optimizer\n",
    "\n",
    "        self.lr, self.B1, self.B2, self.adam_eps = G_lr, G_B1, G_B2, adam_eps\n",
    "\n",
    "        self.optim = optim.Adam(params=self.parameters(), lr=self.lr,\n",
    "                                betas=(self.B1, self.B2), weight_decay=0,\n",
    "                                eps=self.adam_eps)\n",
    "\n",
    "    # Initialize\n",
    "    def init_weights(self):\n",
    "        self.param_count = 0\n",
    "        for module in self.modules():\n",
    "            if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "\n",
    "                init.orthogonal_(module.weight)\n",
    "\n",
    "                self.param_count += sum([p.data.nelement() for p in module.parameters()])\n",
    "        print('Param count for G''s initialized parameters: %d' % self.param_count)\n",
    "\n",
    "    # Note on this forward function: we pass in a y vector which has\n",
    "    # already been passed through G.shared to enable easy class-wise\n",
    "    # interpolation later. If we passed in the one-hot and then ran it through\n",
    "    # G.shared in this forward function, it would be harder to handle.\n",
    "    # NOTE: The z vs y dichotomy here is for compatibility with not-y\n",
    "    def forward(self, z, y):\n",
    "        # concatenate zs and ys\n",
    "        # print('Shape of z',z.shape)\n",
    "        # print('Shape of y',y.shape)\n",
    "        z = torch.cat([y, z], 1)\n",
    "        y = z\n",
    "        # First linear layer\n",
    "        # print('Shape of cont z',z.shape)\n",
    "        h = self.linear(z)\n",
    "        # Reshape\n",
    "        h = h.view(h.size(0), -1, self.bottom_width, self.bottom_width)\n",
    "        # Loop over blocks\n",
    "        for index, blocklist in enumerate(self.blocks):\n",
    "            # Second inner loop in case block has multiple layers\n",
    "            for block in blocklist:\n",
    "                h = block(h, y)\n",
    "\n",
    "        # Apply batchnorm-relu-conv-tanh at output\n",
    "        return torch.tanh(self.output_layer(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f954875930a4237e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.786807Z",
     "start_time": "2024-05-21T14:09:09.776949Z"
    }
   },
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, which_conv=layers.SNConv2d,\n",
    "                 preactivation=True, activation=None, downsample=None,\n",
    "                 channel_ratio=4):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.hidden_channels = self.out_channels // channel_ratio\n",
    "        self.which_conv = which_conv\n",
    "        self.preactivation = preactivation\n",
    "        self.activation = activation\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # Conv layers\n",
    "        self.conv1 = self.which_conv(self.in_channels, self.hidden_channels,\n",
    "                                     kernel_size=1, padding=0)\n",
    "        self.conv2 = self.which_conv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv3 = self.which_conv(self.hidden_channels, self.hidden_channels)\n",
    "        self.conv4 = self.which_conv(self.hidden_channels, self.out_channels,\n",
    "                                     kernel_size=1, padding=0)\n",
    "\n",
    "        self.learnable_sc = True if (in_channels != out_channels) else False\n",
    "        if self.learnable_sc:\n",
    "            self.conv_sc = self.which_conv(in_channels, out_channels - in_channels,\n",
    "                                           kernel_size=1, padding=0)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.downsample(x)\n",
    "        if self.learnable_sc:\n",
    "            x = torch.cat([x, self.conv_sc(x)], 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1x1 bottleneck conv\n",
    "        h = self.conv1(F.relu(x))\n",
    "        # 3x3 convs\n",
    "        h = self.conv2(self.activation(h))\n",
    "        h = self.conv3(self.activation(h))\n",
    "        # relu before downsample\n",
    "        h = self.activation(h)\n",
    "        # downsample\n",
    "        if self.downsample:\n",
    "            h = self.downsample(h)\n",
    "            # final 1x1 conv\n",
    "        h = self.conv4(h)\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "# Discriminator architecture, same paradigm as G's above\n",
    "def D_arch(ch=64, attention='64'):\n",
    "    arch = {}\n",
    "    arch[256] = {'in_channels': [item * ch for item in [1, 2, 4, 8, 8, 16]],\n",
    "                 'out_channels': [item * ch for item in [2, 4, 8, 8, 16, 16]],\n",
    "                 'downsample': [True] * 6 + [False],\n",
    "                 'resolution': [128, 64, 32, 16, 8, 4, 4],\n",
    "                 'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')]\n",
    "                               for i in range(2, 8)}}\n",
    "    arch[128] = {'in_channels': [item * ch for item in [1, 2, 4, 8, 16]],\n",
    "                 'out_channels': [item * ch for item in [2, 4, 8, 16, 16]],\n",
    "                 'downsample': [True] * 5 + [False],\n",
    "                 'resolution': [64, 32, 16, 8, 4, 4],\n",
    "                 'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')]\n",
    "                               for i in range(2, 8)}}\n",
    "    arch[64] = {'in_channels': [item * ch for item in [1, 2, 4, 8]],\n",
    "                'out_channels': [item * ch for item in [2, 4, 8, 16]],\n",
    "                'downsample': [True] * 4 + [False],\n",
    "                'resolution': [32, 16, 8, 4, 4],\n",
    "                'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')]\n",
    "                              for i in range(2, 7)}}\n",
    "    arch[32] = {'in_channels': [item * ch for item in [4, 4, 4]],\n",
    "                'out_channels': [item * ch for item in [4, 4, 4]],\n",
    "                'downsample': [True, True, False, False],\n",
    "                'resolution': [16, 16, 16, 16],\n",
    "                'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')]\n",
    "                              for i in range(2, 6)}}\n",
    "    return arch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "592e320e9d7c20ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T14:09:09.800584Z",
     "start_time": "2024-05-21T14:09:09.786807Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, D_ch=64, D_wide=True, D_depth=2, resolution=32,\n",
    "                 D_kernel_size=3, D_attn='64', n_classes=1000,\n",
    "                 num_D_SVs=1, num_D_SV_itrs=1, D_activation=nn.ReLU(inplace=False),\n",
    "                 D_lr=2e-4, D_B1=0.0, D_B2=0.999, adam_eps=1e-8,\n",
    "                 SN_eps=1e-12, output_dim=1, skip_init=False, **kwargs):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Width multiplier\n",
    "        self.ch = D_ch\n",
    "        # Use Wide D as in BigGAN and SA-GAN or skinny D as in SN-GAN?\n",
    "        self.D_wide = D_wide\n",
    "        # How many resblocks per stage?\n",
    "        self.D_depth = D_depth\n",
    "        # Resolution\n",
    "        self.resolution = resolution\n",
    "        # Kernel size\n",
    "        self.kernel_size = D_kernel_size\n",
    "        # Attention?\n",
    "        self.attention = D_attn\n",
    "        # Number of classes\n",
    "        self.n_classes = n_classes\n",
    "        # Activation\n",
    "        self.activation = D_activation\n",
    "\n",
    "        # Architecture\n",
    "        self.arch = D_arch(self.ch, self.attention)[resolution]\n",
    "\n",
    "        # Which convs, batchnorms, and linear layers to use\n",
    "        # No option to turn off SN in D right now\n",
    "\n",
    "        self.which_conv = functools.partial(layers.SNConv2d, kernel_size=3, padding=1,)\n",
    "        self.which_linear = functools.partial(layers.SNLinear)\n",
    "        self.which_embedding = functools.partial(layers.SNEmbedding)\n",
    "\n",
    "        # Prepare model\n",
    "        # Stem convolution\n",
    "        self.input_conv = self.which_conv(channels, self.arch['in_channels'][0])\n",
    "        # self.blocks is a doubly-nested list of modules, the outer loop intended\n",
    "        # to be over blocks at a given resolution (resblocks and/or self-attention)\n",
    "        self.blocks = []\n",
    "        for index in range(len(self.arch['out_channels'])):\n",
    "            self.blocks += [[DBlock(\n",
    "                in_channels=self.arch['in_channels'][index] if d_index == 0 else self.arch['out_channels'][index],\n",
    "                out_channels=self.arch['out_channels'][index],\n",
    "                which_conv=self.which_conv,\n",
    "                activation=self.activation,\n",
    "                preactivation=True,\n",
    "                downsample=(nn.AvgPool2d(2) if self.arch['downsample'][index] and d_index == 0 else None))\n",
    "                             for d_index in range(self.D_depth)]]\n",
    "            # If attention on this block, attach it to the end\n",
    "            if self.arch['attention'][self.arch['resolution'][index]]:\n",
    "                print('Adding attention layer in D at resolution %d' % self.arch['resolution'][index])\n",
    "                self.blocks[-1] += [layers.Attention(self.arch['out_channels'][index],\n",
    "                                                     self.which_conv)]\n",
    "        # Turn self.blocks into a ModuleList so that it's all properly registered.\n",
    "        self.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "        # Linear output layer. The output dimension is typically 1, but may be\n",
    "        # larger if we're e.g. turning this into a VAE with an inference output\n",
    "        self.linear = self.which_linear(self.arch['out_channels'][-1], output_dim)\n",
    "        # Embedding for projection discrimination\n",
    "        self.embed = self.which_embedding(self.n_classes, self.arch['out_channels'][-1])\n",
    "\n",
    "        # Initialize weights\n",
    "        if not skip_init:\n",
    "            self.init_weights()\n",
    "\n",
    "        # Set up optimizer\n",
    "        self.lr, self.B1, self.B2, self.adam_eps = D_lr, D_B1, D_B2, adam_eps\n",
    "\n",
    "        self.optim = optim.Adam(params=self.parameters(), lr=self.lr,\n",
    "                                betas=(self.B1, self.B2), weight_decay=0, eps=self.adam_eps)\n",
    "\n",
    "\n",
    "    # Initialize\n",
    "    def init_weights(self):\n",
    "        self.param_count = 0\n",
    "        for module in self.modules():\n",
    "            if (isinstance(module, nn.Conv2d)\n",
    "                    or isinstance(module, nn.Linear)\n",
    "                    or isinstance(module, nn.Embedding)):\n",
    "\n",
    "                init.orthogonal_(module.weight)\n",
    "               \n",
    "                self.param_count += sum([p.data.nelement() for p in module.parameters()])\n",
    "        print('Param count for D''s initialized parameters: %d' % self.param_count)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # Run input conv\n",
    "        h = self.input_conv(x)\n",
    "        # Loop over blocks\n",
    "        for index, blocklist in enumerate(self.blocks):\n",
    "            for block in blocklist:\n",
    "                h = block(h)\n",
    "        # Apply global sum pooling as in SN-GAN\n",
    "        h = torch.sum(self.activation(h), [2, 3])\n",
    "        # Get initial class-unconditional output\n",
    "        out = self.linear(h)\n",
    "        # Get projection of final featureset onto class vectors and add to evidence\n",
    "        out = out + torch.sum(self.embed(y) * h, 1, keepdim=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7e52c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45290483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "def save_samples(index, fake_images, show=False):\n",
    "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "    save_image(helpMe.denorm(fake_images[:100]), os.path.join(sample_dir, fake_fname), nrow=10)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(4, 10))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(vu.make_grid(fake_images.cpu().detach(), nrow=10).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9433f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score(real, fake):\n",
    "  real = real.cpu().detach().numpy()\n",
    "  fake = fake.cpu().detach().numpy()\n",
    "\n",
    "  # Ensure NumPy arrays for efficient vectorized operations\n",
    "  real = np.asarray(real)\n",
    "  fake = np.asarray(fake)\n",
    "\n",
    "  # Count elements using vectorized comparison and summation\n",
    "  real_count = np.sum(real >= 1)\n",
    "  fake_count = np.sum(fake <= -1)\n",
    "\n",
    "  # Calculate percentages with handling for empty lists or arrays\n",
    "  real_score = real_count / (len(real) if len(real) > 0 else 1)\n",
    "  fake_score = fake_count / (len(fake) if len(fake) > 0 else 1)\n",
    "\n",
    "  return real_score, fake_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "188c0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count for Gs initialized parameters: 3074177\n",
      "Param count for Ds initialized parameters: 647041\n",
      "Generator(\n",
      "  (activation): ReLU()\n",
      "  (shared): Embedding(10, 128)\n",
      "  (linear): SNLinear(in_features=256, out_features=4096, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-5): 6 x ModuleList(\n",
      "      (0): GBlock(\n",
      "        (activation): ReLU()\n",
      "        (conv1): SNConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv3): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): SNConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): ccbn(\n",
      "          out: 256, in: 256, cross_replica=False\n",
      "          (gain): SNLinear(in_features=256, out_features=256, bias=False)\n",
      "          (bias): SNLinear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (bn2): ccbn(\n",
      "          out: 64, in: 256, cross_replica=False\n",
      "          (gain): SNLinear(in_features=256, out_features=64, bias=False)\n",
      "          (bias): SNLinear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "        (bn3): ccbn(\n",
      "          out: 64, in: 256, cross_replica=False\n",
      "          (gain): SNLinear(in_features=256, out_features=64, bias=False)\n",
      "          (bias): SNLinear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "        (bn4): ccbn(\n",
      "          out: 64, in: 256, cross_replica=False\n",
      "          (gain): SNLinear(in_features=256, out_features=64, bias=False)\n",
      "          (bias): SNLinear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): bn()\n",
      "    (1): ReLU()\n",
      "    (2): SNConv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (activation): ReLU()\n",
      "  (input_conv): SNConv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x ModuleList(\n",
      "      (0): DBlock(\n",
      "        (activation): ReLU()\n",
      "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (conv1): SNConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv3): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): SNConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DBlock(\n",
      "        (activation): ReLU()\n",
      "        (conv1): SNConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv3): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): SNConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0-1): 2 x DBlock(\n",
      "        (activation): ReLU()\n",
      "        (conv1): SNConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv3): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): SNConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): SNLinear(in_features=256, out_features=1, bias=True)\n",
      "  (embed): SNEmbedding(10, 256)\n",
      ")\n",
      "Number of params in G: 3074689 D: 647041\n",
      "Resuming training from epoch 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/11]:   3%|▎         | 16/600 [00:22<13:29,  1.39s/it, D_loss=0.484, G_loss=0.040, Real_score=0.010, Fake_score=0.010]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 159\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of params in G: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m D: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    157\u001b[0m \u001b[38;5;241m*\u001b[39m[\u001b[38;5;28msum\u001b[39m([p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnelement() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mparameters()]) \u001b[38;5;28;01mfor\u001b[39;00m net \u001b[38;5;129;01min\u001b[39;00m [G,D]]))\n\u001b[0;32m    158\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Directory to save or load the checkpoint\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 62\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, dataloader, num_epochs, z_dim, batch_size, checkpoint_dir)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(accumulation_steps):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Generate fake images\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, z_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 62\u001b[0m     fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Combine real and fake images\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     all_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([real_images, fake_images], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[87], line 135\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, z, y)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, blocklist \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Second inner loop in case block has multiple layers\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m blocklist:\n\u001b[1;32m--> 135\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Apply batchnorm-relu-conv-tanh at output\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(h))\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[86], line 40\u001b[0m, in \u001b[0;36mGBlock.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 3x3 convs\u001b[39;00m\n\u001b[0;32m     39\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h)\n\u001b[1;32m---> 40\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Final 1x1 conv\u001b[39;00m\n\u001b[0;32m     42\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(h, y)))\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\pytorch\\expriements\\LAPGAN\\layers.py:309\u001b[0m, in \u001b[0;36mccbn.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m    307\u001b[0m   \u001b[38;5;66;03m# Calculate class-conditional gains and biases\u001b[39;00m\n\u001b[0;32m    308\u001b[0m   gain \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgain(y))\u001b[38;5;241m.\u001b[39mview(y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m   bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    310\u001b[0m   \u001b[38;5;66;03m# If using my batchnorm\u001b[39;00m\n\u001b[0;32m    311\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmybn \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_replica:\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\anaconda3\\envs\\py_torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arun\\pytorch\\expriements\\LAPGAN\\layers.py:122\u001b[0m, in \u001b[0;36mSNLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\Arun\\pytorch\\expriements\\LAPGAN\\layers.py:93\u001b[0m, in \u001b[0;36mSN.W_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Apply num_itrs power iterations\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_itrs):\n\u001b[1;32m---> 93\u001b[0m   svs, us, vs \u001b[38;5;241m=\u001b[39m \u001b[43mpower_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Update the svs\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[1;32mc:\\Users\\Arun\\pytorch\\expriements\\LAPGAN\\layers.py:48\u001b[0m, in \u001b[0;36mpower_iteration\u001b[1;34m(W, u_, update, eps)\u001b[0m\n\u001b[0;32m     46\u001b[0m       u_[i][:] \u001b[38;5;241m=\u001b[39m u\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;66;03m# Compute this singular value and add it to the list\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   svs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39msqueeze(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, u\u001b[38;5;241m.\u001b[39mt()))]\n\u001b[0;32m     49\u001b[0m   \u001b[38;5;66;03m#svs += [torch.sum(F.linear(u, W.transpose(0, 1)) * v)]\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m svs, us, vs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Define the loss function (hinge loss in this example)\n",
    "def loss_hinge_dis(dis_fake, dis_real):\n",
    "    loss_real = torch.mean(F.relu(1. - dis_real))\n",
    "    loss_fake = torch.mean(F.relu(1. + dis_fake))\n",
    "    return loss_real, loss_fake\n",
    "\n",
    "def loss_hinge_gen(dis_fake):\n",
    "    loss = -torch.mean(dis_fake)\n",
    "    return loss\n",
    "\n",
    "# Create a mixed precision optimizer for each network\n",
    "scaler_G = GradScaler()\n",
    "scaler_D = GradScaler()\n",
    "\n",
    "# Define the gradient accumulation steps\n",
    "accumulation_steps = 4\n",
    "\n",
    "# Training function with tqdm\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, z_dim, batch_size, checkpoint_dir=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    \n",
    "    start_epoch = 1\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            generator.optim.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "            discriminator.optim.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "            scaler_G.load_state_dict(checkpoint['scaler_G'])\n",
    "            scaler_D.load_state_dict(checkpoint['scaler_D'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        total_d_loss = 0.0\n",
    "        total_g_loss = 0.0\n",
    "        \n",
    "        with tqdm(enumerate(dataloader), total=len(dataloader)) as t:\n",
    "            for i, (real_images, labels) in t:\n",
    "                real_images = real_images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                # Train discriminator\n",
    "                discriminator.optim.zero_grad()\n",
    "                for a in range(accumulation_steps):\n",
    "                    # Generate fake images\n",
    "                    z = torch.randn(batch_size, z_dim, device=device)\n",
    "                    fake_images = generator(z, generator.shared(labels))\n",
    "                    \n",
    "                    # Combine real and fake images\n",
    "                    all_images = torch.cat([real_images, fake_images], dim=0)\n",
    "                    all_labels = torch.cat([labels, labels], dim=0)\n",
    "                    \n",
    "                    with autocast():\n",
    "                        dis_output = discriminator(all_images, all_labels)\n",
    "                        dis_real, dis_fake = torch.chunk(dis_output, 2)\n",
    "                        loss_real, loss_fake = loss_hinge_dis(dis_fake, dis_real)\n",
    "                        d_loss = (loss_real + loss_fake) / accumulation_steps\n",
    "                        # print(\"rel\",a,dis_real)\n",
    "                        # print(\"fake\",a,dis_fake)\n",
    "                        \n",
    "                    scaler_D.scale(d_loss).backward()\n",
    "                # print(\"disrel\",dis_real)\n",
    "                # print(\"disfake\",dis_fake)\n",
    "                real_score, fake_score = score(dis_real, dis_fake)\n",
    "                # print(\"real score\",real_score)\n",
    "                # print(\"fake score\",fake_score)\n",
    "                # return\n",
    "                scaler_D.step(discriminator.optim)\n",
    "                scaler_D.update()\n",
    "                \n",
    "                total_d_loss += d_loss.item() * accumulation_steps\n",
    "                \n",
    "                # Train generator\n",
    "                generator.optim.zero_grad()\n",
    "                for _ in range(accumulation_steps):\n",
    "                    z = torch.randn(batch_size, z_dim, device=device)\n",
    "                    fake_images = generator(z, generator.shared(labels))\n",
    "                    \n",
    "                    with autocast():\n",
    "                        dis_fake = discriminator(fake_images, labels)\n",
    "                        g_loss = loss_hinge_gen(dis_fake) / accumulation_steps\n",
    "                        \n",
    "                    scaler_G.scale(g_loss).backward()\n",
    "                    \n",
    "                scaler_G.step(generator.optim)\n",
    "                scaler_G.update()\n",
    "                \n",
    "                total_g_loss += g_loss.item() * accumulation_steps\n",
    "                D_loss = total_d_loss / ((i + 1) * accumulation_steps)\n",
    "                G_loss = total_g_loss / ((i + 1) * accumulation_steps)\n",
    "                # Print losses\n",
    "                t.set_description(f'Epoch [{epoch}/{num_epochs}]')\n",
    "                t.set_postfix({'D_loss': f'{D_loss:.3f}',\n",
    "                               'G_loss': f'{G_loss:.3f}',\n",
    "                               'Real_score': f'{real_score:.3f}',\n",
    "                               'Fake_score': f'{fake_score:.3f}'})\n",
    "                \n",
    "                wandb.log({'Loss/Gen': G_loss,\n",
    "                   'Loss/Dis': D_loss,\n",
    "                   'Score/Real': real_score,\n",
    "                   'Score/Fake': fake_score\n",
    "                   })\n",
    "                # Clear unused variables to free up memory\n",
    "                del real_images, labels, fake_images, all_images, all_labels, z, dis_output, dis_real, dis_fake, real_score, fake_score, D_loss, G_loss, a\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Save generated images\n",
    "        # save_generated_images(generator, epoch, batch_size, z_dim, checkpoint_dir, device)\n",
    "        # Save the model\n",
    "        # save_model(generator, discriminator, epoch, checkpoint_dir)\n",
    "\n",
    "def save_generated_images(generator, epoch, batch_size, z_dim, path, device):\n",
    "    os.makedirs(f\"{path}Generated\", exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        fixed_latent = torch.randn(batch_size, z_dim, device=device)\n",
    "        fixed_labels = torch.tensor([i % 10 for i in range(batch_size)], device=device)\n",
    "        fake_images = generator(fixed_latent, generator.shared(fixed_labels))\n",
    "    torchvision.utils.save_image(fake_images.detach(), f\"{path}Generated/generated_images_epoch_{epoch}.png\", normalize=True,nrow=10)\n",
    "\n",
    "def save_model(generator, discriminator, epoch, checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': generator.optim.state_dict(),\n",
    "        'optimizer_D_state_dict': discriminator.optim.state_dict(),\n",
    "        'scaler_G': scaler_G.state_dict(),\n",
    "        'scaler_D': scaler_D.state_dict()\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "train_dl = DataLoader(dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "G = Generator(resolution=image_size, n_classes=len(class_list))\n",
    "D = Discriminator(resolution=image_size, n_classes=len(class_list))\n",
    "\n",
    "print(G)\n",
    "print(D)\n",
    "print('Number of params in G: {} D: {}'.format(\n",
    "*[sum([p.data.nelement() for p in net.parameters()]) for net in [G,D]]))\n",
    "checkpoint_dir = f\"Models/{model_name}/\"  # Directory to save or load the checkpoint\n",
    "train_gan(G, D, train_dl, num_epochs=epochs, z_dim=z_dim, batch_size=batch_size, checkpoint_dir=checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1476dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
